{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarbjeet/git/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'DQNmodel_Output/cartpole/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        \n",
    "    def _build_model(self) :\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(24, input_dim = state_size, activation = 'relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation = 'linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate)) \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.model.fit(state,target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load (self,name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save (self,name):\n",
    "        self.model.save_weights(name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1001, score: 18, e: 1.0\n",
      "episode: 1/1001, score: 13, e: 1.0\n",
      "episode: 2/1001, score: 21, e: 0.99\n",
      "episode: 3/1001, score: 18, e: 0.99\n",
      "episode: 4/1001, score: 23, e: 0.99\n",
      "episode: 5/1001, score: 28, e: 0.98\n",
      "episode: 6/1001, score: 24, e: 0.98\n",
      "episode: 7/1001, score: 11, e: 0.97\n",
      "episode: 8/1001, score: 14, e: 0.97\n",
      "episode: 9/1001, score: 13, e: 0.96\n",
      "episode: 10/1001, score: 29, e: 0.96\n",
      "episode: 11/1001, score: 10, e: 0.95\n",
      "episode: 12/1001, score: 63, e: 0.95\n",
      "episode: 13/1001, score: 16, e: 0.94\n",
      "episode: 14/1001, score: 39, e: 0.94\n",
      "episode: 15/1001, score: 25, e: 0.93\n",
      "episode: 16/1001, score: 29, e: 0.93\n",
      "episode: 17/1001, score: 20, e: 0.92\n",
      "episode: 18/1001, score: 9, e: 0.92\n",
      "episode: 19/1001, score: 19, e: 0.91\n",
      "episode: 20/1001, score: 15, e: 0.91\n",
      "episode: 21/1001, score: 9, e: 0.9\n",
      "episode: 22/1001, score: 40, e: 0.9\n",
      "episode: 23/1001, score: 25, e: 0.9\n",
      "episode: 24/1001, score: 13, e: 0.89\n",
      "episode: 25/1001, score: 33, e: 0.89\n",
      "episode: 26/1001, score: 12, e: 0.88\n",
      "episode: 27/1001, score: 19, e: 0.88\n",
      "episode: 28/1001, score: 30, e: 0.87\n",
      "episode: 29/1001, score: 14, e: 0.87\n",
      "episode: 30/1001, score: 15, e: 0.86\n",
      "episode: 31/1001, score: 11, e: 0.86\n",
      "episode: 32/1001, score: 14, e: 0.86\n",
      "episode: 33/1001, score: 17, e: 0.85\n",
      "episode: 34/1001, score: 11, e: 0.85\n",
      "episode: 35/1001, score: 15, e: 0.84\n",
      "episode: 36/1001, score: 41, e: 0.84\n",
      "episode: 37/1001, score: 57, e: 0.83\n",
      "episode: 38/1001, score: 32, e: 0.83\n",
      "episode: 39/1001, score: 9, e: 0.83\n",
      "episode: 40/1001, score: 9, e: 0.82\n",
      "episode: 41/1001, score: 12, e: 0.82\n",
      "episode: 42/1001, score: 9, e: 0.81\n",
      "episode: 43/1001, score: 16, e: 0.81\n",
      "episode: 44/1001, score: 14, e: 0.81\n",
      "episode: 45/1001, score: 24, e: 0.8\n",
      "episode: 46/1001, score: 10, e: 0.8\n",
      "episode: 47/1001, score: 10, e: 0.79\n",
      "episode: 48/1001, score: 9, e: 0.79\n",
      "episode: 49/1001, score: 22, e: 0.79\n",
      "episode: 50/1001, score: 25, e: 0.78\n",
      "episode: 51/1001, score: 17, e: 0.78\n",
      "episode: 52/1001, score: 22, e: 0.77\n",
      "episode: 53/1001, score: 23, e: 0.77\n",
      "episode: 54/1001, score: 11, e: 0.77\n",
      "episode: 55/1001, score: 13, e: 0.76\n",
      "episode: 56/1001, score: 9, e: 0.76\n",
      "episode: 57/1001, score: 12, e: 0.76\n",
      "episode: 58/1001, score: 8, e: 0.75\n",
      "episode: 59/1001, score: 30, e: 0.75\n",
      "episode: 60/1001, score: 15, e: 0.74\n",
      "episode: 61/1001, score: 26, e: 0.74\n",
      "episode: 62/1001, score: 49, e: 0.74\n",
      "episode: 63/1001, score: 21, e: 0.73\n",
      "episode: 64/1001, score: 7, e: 0.73\n",
      "episode: 65/1001, score: 19, e: 0.73\n",
      "episode: 66/1001, score: 26, e: 0.72\n",
      "episode: 67/1001, score: 16, e: 0.72\n",
      "episode: 68/1001, score: 68, e: 0.71\n",
      "episode: 69/1001, score: 18, e: 0.71\n",
      "episode: 70/1001, score: 39, e: 0.71\n",
      "episode: 71/1001, score: 31, e: 0.7\n",
      "episode: 72/1001, score: 21, e: 0.7\n",
      "episode: 73/1001, score: 38, e: 0.7\n",
      "episode: 74/1001, score: 30, e: 0.69\n",
      "episode: 75/1001, score: 38, e: 0.69\n",
      "episode: 76/1001, score: 29, e: 0.69\n",
      "episode: 77/1001, score: 18, e: 0.68\n",
      "episode: 78/1001, score: 29, e: 0.68\n",
      "episode: 79/1001, score: 49, e: 0.68\n",
      "episode: 80/1001, score: 16, e: 0.67\n",
      "episode: 81/1001, score: 11, e: 0.67\n",
      "episode: 82/1001, score: 31, e: 0.67\n",
      "episode: 83/1001, score: 25, e: 0.66\n",
      "episode: 84/1001, score: 84, e: 0.66\n",
      "episode: 85/1001, score: 18, e: 0.66\n",
      "episode: 86/1001, score: 25, e: 0.65\n",
      "episode: 87/1001, score: 62, e: 0.65\n",
      "episode: 88/1001, score: 38, e: 0.65\n",
      "episode: 89/1001, score: 31, e: 0.64\n",
      "episode: 90/1001, score: 13, e: 0.64\n",
      "episode: 91/1001, score: 24, e: 0.64\n",
      "episode: 92/1001, score: 15, e: 0.63\n",
      "episode: 93/1001, score: 52, e: 0.63\n",
      "episode: 94/1001, score: 30, e: 0.63\n",
      "episode: 95/1001, score: 12, e: 0.62\n",
      "episode: 96/1001, score: 33, e: 0.62\n",
      "episode: 97/1001, score: 23, e: 0.62\n",
      "episode: 98/1001, score: 54, e: 0.61\n",
      "episode: 99/1001, score: 27, e: 0.61\n",
      "episode: 100/1001, score: 41, e: 0.61\n",
      "episode: 101/1001, score: 31, e: 0.61\n",
      "episode: 102/1001, score: 69, e: 0.6\n",
      "episode: 103/1001, score: 65, e: 0.6\n",
      "episode: 104/1001, score: 43, e: 0.6\n",
      "episode: 105/1001, score: 43, e: 0.59\n",
      "episode: 106/1001, score: 40, e: 0.59\n",
      "episode: 107/1001, score: 11, e: 0.59\n",
      "episode: 108/1001, score: 35, e: 0.58\n",
      "episode: 109/1001, score: 47, e: 0.58\n",
      "episode: 110/1001, score: 55, e: 0.58\n",
      "episode: 111/1001, score: 40, e: 0.58\n",
      "episode: 112/1001, score: 69, e: 0.57\n",
      "episode: 113/1001, score: 49, e: 0.57\n",
      "episode: 114/1001, score: 32, e: 0.57\n",
      "episode: 115/1001, score: 23, e: 0.56\n",
      "episode: 116/1001, score: 19, e: 0.56\n",
      "episode: 117/1001, score: 20, e: 0.56\n",
      "episode: 118/1001, score: 23, e: 0.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d5f0cb3ed5a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gym/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gym/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gym/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \"\"\"\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1,state_size])\n",
    "    \n",
    "    for time in range(5000):\n",
    "        \n",
    "        env.render() \n",
    "        action = agent.act(state) \n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        reward = reward if not done else -10\n",
    "        \n",
    "        next_state = np.reshape(next_state, [1,state_size])\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done) \n",
    "        \n",
    "        state = next_state \n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, n_episodes, time, agent.epsilon))\n",
    "            break\n",
    "        \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size) \n",
    "    \n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + \"weights_\"+'{:04d}'.format(e)+'.hsf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
